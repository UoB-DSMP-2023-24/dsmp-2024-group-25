{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataset import MyDataset\n",
    "from LSTM import LSTMModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liubohan/Documents/GitHub/dsmp-2024-group-25/Final-term for the project/LSTM/dataset.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
      "/Users/liubohan/Documents/GitHub/dsmp-2024-group-25/Final-term for the project/LSTM/dataset.py:41: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  grouped = data.groupby(pd.Grouper(key='Timestamp', freq='2H'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6486, -5.2804],\n",
       "         [-0.6505,  0.4418],\n",
       "         [-0.6747, -0.1769],\n",
       "         [-0.7913, -5.2804],\n",
       "         [-0.7277, -5.2804],\n",
       "         [-0.6319, -5.2804],\n",
       "         [-0.6001, -5.2804],\n",
       "         [-0.5581, -5.2804],\n",
       "         [-0.6494, -5.2804],\n",
       "         [-0.5717, -5.2804],\n",
       "         [-0.6471, -5.2804],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000]]),\n",
       " tensor([0.8182, 0.0909, 0.0000, 0.0000, 0.0000, 0.0000, 0.0909, 0.0000, 0.0000,\n",
       "         0.0000]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = MyDataset(file_path='/Users/liubohan/Documents/GitHub/dsmp-2024-group-25/Final-term for the project/LSTM/Train.csv')\n",
    "\n",
    "# data.__getitem__(0)\n",
    "# data.getmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input size, hidden size, number of layers, and output size\n",
    "input_size = 2\n",
    "hidden_size = 64\n",
    "num_layers = 4\n",
    "output_size = 10\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.9377107046827484\n",
      "Epoch 2/100, Loss: 1.9378556895859633\n",
      "Epoch 3/100, Loss: 1.9377170435235471\n",
      "Epoch 4/100, Loss: 1.9378058106838902\n",
      "Epoch 5/100, Loss: 1.9382012086578562\n",
      "Epoch 6/100, Loss: 1.9379548966130125\n",
      "Epoch 7/100, Loss: 1.9379112961926037\n",
      "Epoch 8/100, Loss: 1.9376065383983563\n",
      "Epoch 9/100, Loss: 1.937452101254765\n",
      "Epoch 10/100, Loss: 1.9378140312206895\n",
      "Epoch 11/100, Loss: 1.9365207103988793\n",
      "Epoch 12/100, Loss: 1.9360266060014315\n",
      "Epoch 13/100, Loss: 1.9360636582102957\n",
      "Epoch 14/100, Loss: 1.9360036238839355\n",
      "Epoch 15/100, Loss: 1.9360371224487885\n",
      "Epoch 16/100, Loss: 1.9361111521720886\n",
      "Epoch 17/100, Loss: 1.9359997484502913\n",
      "Epoch 18/100, Loss: 1.9360089996193028\n",
      "Epoch 19/100, Loss: 1.9360301951064338\n",
      "Epoch 20/100, Loss: 1.9359695768054528\n",
      "Epoch 21/100, Loss: 1.935872330695768\n",
      "Epoch 22/100, Loss: 1.935846196699746\n",
      "Epoch 23/100, Loss: 1.9358262525329106\n",
      "Epoch 24/100, Loss: 1.9358188087427164\n",
      "Epoch 25/100, Loss: 1.9358096247232413\n",
      "Epoch 26/100, Loss: 1.9358061604861971\n",
      "Epoch 27/100, Loss: 1.935814315759683\n",
      "Epoch 28/100, Loss: 1.935807908637614\n",
      "Epoch 29/100, Loss: 1.9358106281938432\n",
      "Epoch 30/100, Loss: 1.9358050408242624\n",
      "Epoch 31/100, Loss: 1.935779859370823\n",
      "Epoch 32/100, Loss: 1.9357791799533217\n",
      "Epoch 33/100, Loss: 1.9357791467557979\n",
      "Epoch 34/100, Loss: 1.9357784492305563\n",
      "Epoch 35/100, Loss: 1.93577889362468\n",
      "Epoch 36/100, Loss: 1.9357793814019313\n",
      "Epoch 37/100, Loss: 1.935779339527782\n",
      "Epoch 38/100, Loss: 1.9357793485816521\n",
      "Epoch 39/100, Loss: 1.935779031696199\n",
      "Epoch 40/100, Loss: 1.935778967564619\n",
      "Epoch 41/100, Loss: 1.9357760657992544\n",
      "Epoch 42/100, Loss: 1.9357760774938366\n",
      "Epoch 43/100, Loss: 1.9357761386074597\n",
      "Epoch 44/100, Loss: 1.935776091829131\n",
      "Epoch 45/100, Loss: 1.9357761242721654\n",
      "Epoch 46/100, Loss: 1.935776151056531\n",
      "Epoch 47/100, Loss: 1.9357761050326914\n",
      "Epoch 48/100, Loss: 1.9357760865477067\n",
      "Epoch 49/100, Loss: 1.9357760680627218\n",
      "Epoch 50/100, Loss: 1.9357761061644252\n",
      "Epoch 51/100, Loss: 1.9357758398297467\n",
      "Epoch 52/100, Loss: 1.9357758443566817\n",
      "Epoch 53/100, Loss: 1.9357758356800563\n",
      "Epoch 54/100, Loss: 1.935775849638106\n",
      "Epoch 55/100, Loss: 1.9357758345483225\n",
      "Epoch 56/100, Loss: 1.9357758447339264\n",
      "Epoch 57/100, Loss: 1.9357758345483225\n",
      "Epoch 58/100, Loss: 1.9357758394525022\n",
      "Epoch 59/100, Loss: 1.9357758356800563\n",
      "Epoch 60/100, Loss: 1.9357758394525022\n",
      "Epoch 61/100, Loss: 1.9357758722727811\n",
      "Epoch 62/100, Loss: 1.9357758545422856\n",
      "Epoch 63/100, Loss: 1.9357758292668983\n",
      "Epoch 64/100, Loss: 1.935775802105288\n",
      "Epoch 65/100, Loss: 1.9357758190812944\n",
      "Epoch 66/100, Loss: 1.9357757934286624\n",
      "Epoch 67/100, Loss: 1.935775812290892\n",
      "Epoch 68/100, Loss: 1.9357758160633376\n",
      "Epoch 69/100, Loss: 1.9357758171950714\n",
      "Epoch 70/100, Loss: 1.935775806632223\n",
      "Epoch 71/100, Loss: 1.9357758386980128\n",
      "Epoch 72/100, Loss: 1.9357758417159696\n",
      "Epoch 73/100, Loss: 1.9357758443566817\n",
      "Epoch 74/100, Loss: 1.9357758311531212\n",
      "Epoch 75/100, Loss: 1.9357758322848548\n",
      "Epoch 76/100, Loss: 1.9357758398297467\n",
      "Epoch 77/100, Loss: 1.9357758345483225\n",
      "Epoch 78/100, Loss: 1.9357758383207684\n",
      "Epoch 79/100, Loss: 1.9357758439794372\n",
      "Epoch 80/100, Loss: 1.9357758390752575\n",
      "Epoch 81/100, Loss: 1.9357758345483225\n",
      "Epoch 82/100, Loss: 1.9357758398297467\n",
      "Epoch 83/100, Loss: 1.9357758379435237\n",
      "Epoch 84/100, Loss: 1.9357758266261862\n",
      "Epoch 85/100, Loss: 1.9357758349255672\n",
      "Epoch 86/100, Loss: 1.9357758405842358\n",
      "Epoch 87/100, Loss: 1.9357758277579198\n",
      "Epoch 88/100, Loss: 1.9357758386980128\n",
      "Epoch 89/100, Loss: 1.9357758428477034\n",
      "Epoch 90/100, Loss: 1.9357758394525022\n",
      "Epoch 91/100, Loss: 1.9357758356800563\n",
      "Epoch 92/100, Loss: 1.935775841338725\n",
      "Epoch 93/100, Loss: 1.9357758315303657\n",
      "Epoch 94/100, Loss: 1.9357758451111708\n",
      "Epoch 95/100, Loss: 1.9357758454884155\n",
      "Epoch 96/100, Loss: 1.9357758375662792\n",
      "Epoch 97/100, Loss: 1.9357758375662792\n",
      "Epoch 98/100, Loss: 1.9357758394525022\n",
      "Epoch 99/100, Loss: 1.9357758341710778\n",
      "Epoch 100/100, Loss: 1.9357758341710778\n",
      "Finished Training\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Define number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define a learning rate scheduler for dynamic adjustment of learning rate\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print loss every epoch\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}')\n",
    "\n",
    "print('Finished Training')\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'lstm_model.pth')\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 使用与训练时相同的模型初始化参数\n",
    "\n",
    "model.load_state_dict(torch.load('lstm_model.pth'))\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liubohan/Documents/GitHub/dsmp-2024-group-25/Final-term for the project/LSTM/dataset.py:38: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
      "/Users/liubohan/Documents/GitHub/dsmp-2024-group-25/Final-term for the project/LSTM/dataset.py:41: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  grouped = data.groupby(pd.Grouper(key='Timestamp', freq='2H'))\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = MyDataset(file_path='/Users/liubohan/Documents/GitHub/dsmp-2024-group-25/Final-term for the project/LSTM/Valid.csv')\n",
    "val_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 2.417806976397518\n",
      "Accuracy: 33.48496835443038\n",
      "Epoch 2/100, Loss: 2.4178069599061143\n",
      "Accuracy: 33.48496835443038\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Accumulate loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/text_analytics/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/text_analytics/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(val_dataloader):\n",
    "\n",
    "        _, targets_indices = torch.max(targets, 1) \n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "        total += targets_indices.size(0)                  # Increment the total count\n",
    "        correct += (predicted == targets_indices).sum().item() \n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_val_loss/len(val_dataloader)}')\n",
    "    print(f'Accuracy: {100 * correct / total}')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_analytics",
   "language": "python",
   "name": "text_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
